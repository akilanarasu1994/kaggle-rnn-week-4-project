{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport regex as re\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.stem.regexp import RegexpStemmer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing .sequence import pad_sequences\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv1D, BatchNormalization, Dropout, Dense, MaxPooling1D, Activation, Flatten, LSTM, GRU,Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-21T22:39:41.558077Z","iopub.execute_input":"2022-06-21T22:39:41.558493Z","iopub.status.idle":"2022-06-21T22:39:41.566532Z","shell.execute_reply.started":"2022-06-21T22:39:41.558458Z","shell.execute_reply":"2022-06-21T22:39:41.565664Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '../input/nlp-getting-started/train.csv'\nTEST_PATH = '../input/nlp-getting-started/test.csv'\n\nRULE_DIGITS = r'[0-9]+'\nRULE_MENTIONS = r'@mention'\nRULE_HTTP = r'https?:\\/\\/\\S+'\nRULE_URLS = r\"www.\\[a-z]?\\.?(com)+|[a-z]+\\.(com)\"\nRULE_SYMBOLS = r\"[_\\,\\>\\(\\-:\\)\\\\\\/\\!\\.\\?\\@\\$\\];='#]\"\n\nSTEMMER_RULE = 'ing$|s$|es$|able$|d$'","metadata":{"execution":{"iopub.status.busy":"2022-06-21T22:40:33.676351Z","iopub.execute_input":"2022-06-21T22:40:33.676801Z","iopub.status.idle":"2022-06-21T22:40:33.683478Z","shell.execute_reply.started":"2022-06-21T22:40:33.676767Z","shell.execute_reply":"2022-06-21T22:40:33.682054Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_PATH)\ntest_data = pd.read_csv(TEST_PATH)\n\ny = train_data['target']\ntrain = train_data['text']\ntest = test_data['text']\n\n# data preprocessing\ntrain = train.str.lower()\ntrain = train.apply(lambda x: re.sub(RULE_DIGITS,'',x))\ntrain = train.apply(lambda x: re.sub(RULE_MENTIONS,'',x))\ntrain = train.apply(lambda x: re.sub(RULE_HTTP, '',x))\ntrain = train.apply(lambda x: re.sub(RULE_URLS, '',x))\ntrain = train.apply(lambda x: re.sub(RULE_SYMBOLS,'',x))\n\ntest = test.str.lower()\ntest = test.apply(lambda x: re.sub(RULE_DIGITS,'',x))\ntest = test.apply(lambda x: re.sub(RULE_MENTIONS,'',x))\ntest = test.apply(lambda x: re.sub(RULE_HTTP, '',x))\ntest = test.apply(lambda x: re.sub(RULE_URLS, '',x))\ntest = test.apply(lambda x: re.sub(RULE_SYMBOLS,'',x))\n\ntokenizer = TreebankWordTokenizer()\nstemmer = RegexpStemmer(STEMMER_RULE, min=5) # Removes morphological affixes\ntoken = train.apply(tokenizer.tokenize)\nwords = [[stemmer.stem(tok) for tok in tokens] for tokens in token]\ntrain = [' '.join(word) for word in words]\nprint(train[:9])\n\ntoken1 = test.apply(tokenizer.tokenize)\nwords1 = [[stemmer.stem(tok) for tok in tokens] for tokens in token1]\ntest = [' '.join(word) for word in words1]\nprint(test[:9])","metadata":{"execution":{"iopub.status.busy":"2022-06-21T22:40:33.802035Z","iopub.execute_input":"2022-06-21T22:40:33.802415Z","iopub.status.idle":"2022-06-21T22:40:36.385484Z","shell.execute_reply.started":"2022-06-21T22:40:33.802385Z","shell.execute_reply":"2022-06-21T22:40:36.384370Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\ndef encode_docs(tokenizer, max_length, docs):\n    encoded = tokenizer.texts_to_sequences(docs)\n    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n    return padded\n\n# define the model\ndef define_model(vocab_size, max_length):\n    model=keras.Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n    model.add(Bidirectional(GRU(32,dropout=0.2,recurrent_dropout=0.1,return_sequences=True)))\n    model.add(Bidirectional(GRU(32,dropout=0.2,recurrent_dropout=0.1)))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-06-21T22:40:36.387303Z","iopub.execute_input":"2022-06-21T22:40:36.387671Z","iopub.status.idle":"2022-06-21T22:40:36.398018Z","shell.execute_reply.started":"2022-06-21T22:40:36.387637Z","shell.execute_reply":"2022-06-21T22:40:36.396802Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"tokenizer = create_tokenizer(train)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary size: %d' % vocab_size)\nmax_length = max([len(s.split()) for s in train])\nprint('Maximum length: %d' % max_length)\nXtrain = encode_docs(tokenizer, max_length, train)\nprint(Xtrain.shape)\n\ntokenizer1 = create_tokenizer(test)\nvocab_size1 = len(tokenizer1.word_index) + 1\nprint('Vocabulary size: %d' % vocab_size1)\nmax_length1 = max([len(s.split()) for s in test])\nprint('Maximum length: %d' % max_length1)\ntest = encode_docs(tokenizer1, max_length1, test)\nprint(test.shape)\n\nXtrain,valid_x,y,valid_y=train_test_split(Xtrain,y,test_size=0.25)\n\nmodel = define_model(vocab_size, max_length)\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=20,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    Xtrain, y,\n    validation_data=(valid_x, valid_y),\n    batch_size=512,\n    epochs=1000,\n    callbacks=[early_stopping],\n    verbose=0, # hide the output because we have so many epochs\n)\n\nhistory_df = pd.DataFrame(history.history)\n# Start to plot at epoch 5\nhistory_df.loc[5:, ['loss', 'val_loss']].plot()\nhistory_df.loc[5:, ['accuracy', 'val_accuracy']].plot()\nplt.show()\n\nprint((\"Best Loss: {:0.4f}\" + \"\\nBest Accuracy: {:0.4f}\")\\\n        .format(history_df['loss'].min(),history_df['accuracy'].max()))\nprint((\"Best Validation Loss: {:0.4f}\" + \"\\nBest Validation Accuracy: {:0.4f}\")\\\n        .format(history_df['val_loss'].min(),history_df['val_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2022-06-21T22:40:36.399425Z","iopub.execute_input":"2022-06-21T22:40:36.399783Z","iopub.status.idle":"2022-06-21T22:43:52.694073Z","shell.execute_reply.started":"2022-06-21T22:40:36.399752Z","shell.execute_reply":"2022-06-21T22:43:52.692574Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"ids = test_data['id']\ntest_pred = model.predict(test)\ntest_pred = [round(x[0]) for x in test_pred]\ntest_pred = list(zip(ids,test_pred))\nresult = pd.DataFrame(test_pred,columns = ['id','target'])\nresult = result.set_index('id').to_csv('./pred.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-21T22:43:52.696743Z","iopub.execute_input":"2022-06-21T22:43:52.697209Z","iopub.status.idle":"2022-06-21T22:44:00.323960Z","shell.execute_reply.started":"2022-06-21T22:43:52.697163Z","shell.execute_reply":"2022-06-21T22:44:00.323003Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}